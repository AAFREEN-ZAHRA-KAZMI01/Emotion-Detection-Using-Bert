# -*- coding: utf-8 -*-
"""EMOTION-DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1geS3cAR4NjB9bavR9_ed9WDBwP0CibVo

STEP 1: Libraries Install & Import
"""

!pip install transformers gradio torch pandas scikit-learn

import torch
from transformers import AutoTokenizer, AutoModel
import pandas as pd
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import gradio as gr

"""STEP 2: Load & Preprocess Dataset"""

# Load your dataset (update the path if needed)
df = pd.read_csv('/content/go_emotions_dataset.csv')

# List all emotion columns (as per your dataframe)
emotion_columns = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',
    'remorse', 'sadness', 'surprise', 'neutral'
]

# Remove NaN texts
df = df.dropna(subset=['text'])

# Features & labels
X = df['text'].tolist()
y = df[emotion_columns].values.astype(np.float32)

# Split into train and validation
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.1, random_state=42)

"""STEP 3: Tokenizer & Dataset Loader"""

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
MAX_LEN = 64

class GoEmotionsDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        encoding = tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item['labels'] = torch.FloatTensor(self.labels[idx])
        return item

train_dataset = GoEmotionsDataset(X_train, y_train)
val_dataset = GoEmotionsDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

"""STEP 4: Model Setup"""

import torch.nn as nn
class MultiLabelBERT(nn.Module):
    def __init__(self, num_labels):
        super().__init__()
        self.bert = AutoModel.from_pretrained("bert-base-uncased")
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(cls_output)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = len(emotion_columns)
model = MultiLabelBERT(num_labels).to(device)

"""STEP 5: Training Loop"""

import torch.optim as optim

optimizer = optim.AdamW(model.parameters(), lr=2e-5)
loss_fn = nn.BCEWithLogitsLoss()
epochs = 1  # Try 2 first, increase if needed

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        logits = model(input_ids, attention_mask)
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")

from google.colab import files
torch.save(model.state_dict(), "/content/model_trained.pth")
files.download("/content/model_trained.pth")

"""Code to Evaluate on Validation Set and Plot Confusion Matrix per Label


"""

from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score, multilabel_confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].cpu().numpy()

        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs > 0.5).astype(int)

        all_preds.append(preds)
        all_labels.append(labels)

all_preds = np.vstack(all_preds)
all_labels = np.vstack(all_labels)

# Metrics
print("Hamming Loss:", hamming_loss(all_labels, all_preds))
print("Micro F1 Score:", f1_score(all_labels, all_preds, average='micro'))
print("Macro F1 Score:", f1_score(all_labels, all_preds, average='macro'))
print("Micro Precision:", precision_score(all_labels, all_preds, average='micro'))
print("Micro Recall:", recall_score(all_labels, all_preds, average='micro'))

# Confusion matrices per class
cm = multilabel_confusion_matrix(all_labels, all_preds)

# Plot confusion matrix for a few emotions
def plot_confusion_matrix_per_label(cm, labels, indices):
    for i in indices:
        tn, fp, fn, tp = cm[i].ravel()
        fig, ax = plt.subplots()
        matrix = np.array([[tp, fp],[fn, tn]])
        ax.matshow(matrix, cmap=plt.cm.Blues, alpha=0.7)
        for (j,k), val in np.ndenumerate(matrix):
            ax.text(k, j, val, ha='center', va='center')
        ax.set_xticklabels(['', 'Predicted Positive', 'Predicted Negative'])
        ax.set_yticklabels(['', 'Actual Positive', 'Actual Negative'])
        plt.title(f'Confusion Matrix for "{labels[i]}"')
        plt.show()

# Plot confusion matrices for some emotions (e.g. joy, sadness, anger)
indices_to_plot = [emotion_columns.index(e) for e in ['joy', 'sadness', 'anger']]
plot_confusion_matrix_per_label(cm, emotion_columns, indices_to_plot)

"""Manual Testing of Examples"""

test_sentences = [
    "I am so happy and grateful for your help!",
    "I feel so sad and alone right now.",
    "I am very angry and disappointed.",
    "Wow! That was such a surprise!",
    "I am confused but curious."
]

def predict(text):
    model.eval()
    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=64)
    input_ids = tokens['input_ids'].to(device)
    attention_mask = tokens['attention_mask'].to(device)
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()[0]
    pred_labels = [idx2emotion[i] for i, p in enumerate(probs) if p > 0.5]
    return pred_labels

for sent in test_sentences:
    preds = predict(sent)
    print(f"Text: {sent}")
    print(f"Predicted Emotions: {preds}")
    print("-" * 50)

"""STEP 6: Inference Helpers"""

idx2emotion = {i: e for i, e in enumerate(emotion_columns)}
emotion2emoji = {
    'admiration': "ğŸ‘", 'amusement': "ğŸ˜‚", 'anger': "ğŸ˜¡", 'annoyance': "ğŸ˜’", 'approval': "ğŸ‘",
    'caring': "ğŸ¤—", 'confusion': "ğŸ˜•", 'curiosity': "ğŸ¤”", 'desire': "ğŸ˜", 'disappointment': "ğŸ˜",
    'disapproval': "ğŸ‘", 'disgust': "ğŸ¤¢", 'embarrassment': "ğŸ˜³", 'excitement': "ğŸ¤©", 'fear': "ğŸ˜±",
    'gratitude': "ğŸ™", 'grief': "ğŸ˜­", 'joy': "ğŸ˜ƒ", 'love': "â¤ï¸", 'nervousness': "ğŸ˜¬", 'optimism': "ğŸ˜Š",
    'pride': "ğŸ˜Œ", 'realization': "ğŸ’¡", 'relief': "ğŸ˜Œ", 'remorse': "ğŸ˜”", 'sadness': "ğŸ˜¢",
    'surprise': "ğŸ˜®", 'neutral': "ğŸ˜"
}

"""STEP 7: Gradio Interface for Prediction"""

# Load trained weights (after training)
model.load_state_dict(torch.load("/content/model_trained.pth", map_location=device))
model.eval()

def predict_emotions(text, threshold=0.5):
    tokens = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=MAX_LEN
    )
    input_ids = tokens['input_ids'].to(device)
    attention_mask = tokens['attention_mask'].to(device)
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()[0]
    pred_labels = [idx2emotion[i] for i, p in enumerate(probs) if p > threshold]
    if pred_labels:
        result = " | ".join([f"{emotion2emoji.get(e, '')} {e.capitalize()}" for e in pred_labels])
    else:
        result = "ğŸ˜ Neutral"
    return result

iface = gr.Interface(
    fn=predict_emotions,
    inputs=gr.Textbox(lines=2, placeholder="Type your text here..."),
    outputs="text",
    title="ğŸ­ Multi-Label Emotion Recognition",
    description="Detect multiple emotions present in your text. Each emotion is shown with an emoji. Powered by BERT!"
)
iface.launch(share=True)